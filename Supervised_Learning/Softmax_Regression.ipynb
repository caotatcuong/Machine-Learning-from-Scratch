{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax_Regression:\n",
    "    def __init__(self, alpha=0.0001, max_iter=1000, l2=0.01, tol = 1e-4):\n",
    "        ''' \n",
    "        Class constructor\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha: the learning rate determines how big the step would be on each iteration.\n",
    "        max_iter: number of times update weight\n",
    "        l2: l2 regularization\n",
    "        tol: weight threshold changes to stopping.\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.l2 = l2\n",
    "        self.tol = tol\n",
    "        self.weight = None\n",
    "    \n",
    "    def softmax(self, z):\n",
    "        '''\n",
    "        Computes softmax function for each row of array S.\n",
    "        '''\n",
    "        A = np.exp(z - np.max(z, axis = 1, keepdims = True))\n",
    "        A /= A.sum(axis = 1, keepdims=True)\n",
    "\n",
    "        return A\n",
    "    \n",
    "    def one_hot_encoding(self, Y, num_classes):\n",
    "        '''\n",
    "        Y to one hot encoding\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        y: numpy array, shape (m, 1) \n",
    "        The vector of outputs.\n",
    "        \n",
    "        Return\n",
    "        y: numpy array, shape (m, num_classes) \n",
    "        '''\n",
    "        one_hot_Y = np.zeros((len(Y), num_classes))\n",
    "        one_hot_Y[np.arange(len(Y)), Y.reshape(-1)] = 1\n",
    "        return one_hot_Y\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains Softmax Regression on the dataset (X, y) using Stochastic Gradient Descent.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array, shape (m, n)\n",
    "        The matrix of inputs\n",
    "        y : numpy array, shape (m, 1) \n",
    "        The vector of outputs.\n",
    "        '''\n",
    "            \n",
    "        # First column of this matrix is all ones (corresponding to x_0).\n",
    "        X = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
    "        m, n = X.shape\n",
    "        \n",
    "        # Initialize weights with shape (n + 1, num_classes) \n",
    "        np.random.seed(0) \n",
    "        limit = 1 / math.sqrt(n)\n",
    "        W = [np.random.uniform(-limit, limit, (n, len(np.unique(y))))]\n",
    "        \n",
    "        # Checking for changes in weight after 20 iter \n",
    "        check_w_after = 20\n",
    "        \n",
    "        y = self.one_hot_encoding(y, len(np.unique(y)))\n",
    "\n",
    "        for iter in range(1, self.max_iter + 1):\n",
    "            # mix data \n",
    "            mix_id = np.random.permutation(m)\n",
    "            for i in mix_id:\n",
    "                w_new = W[-1]\n",
    "                error = self.softmax(X[i].reshape(1,-1) @ W[-1]) - y[i]\n",
    "                grad = X[i].reshape(1,-1).T.dot(error) + self.l2 * W[-1]\n",
    "                w_new -= self.alpha * grad\n",
    "\n",
    "                #Early stopping\n",
    "                if iter%check_w_after == 0:                \n",
    "                    if np.linalg.norm(w_new - W[-check_w_after]) < self.tol:\n",
    "                        break \n",
    "                        \n",
    "                W.append(w_new)\n",
    "        self.weight = W[-1]\n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict using the Softmax Regression model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array, shape (m, n)\n",
    "        The matrix of inputs\n",
    "        \n",
    "        Return\n",
    "        ----------\n",
    "        Returns predicted values.\n",
    "        '''\n",
    "        # First column of this matrix is all ones (corresponding to x_0).\n",
    "        X = np.append(np.ones((X.shape[0], 1)), X, axis = 1)    \n",
    "        return np.argmax(self.softmax(X @ self.weight), axis = 1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardScaler(X):\n",
    "    return (X - np.mean(X)) / np.std(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1,1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(45, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Softmax_Regression()\n",
    "\n",
    "X_train = standardScaler(X_train)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_test = standardScaler(X_test)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No using l2\n",
    "model = Softmax_Regression(alpha=0.1, max_iter=5000, l2=0)\n",
    "\n",
    "X_train = standardScaler(X_train)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_test = standardScaler(X_test)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using l2 = 0.1\n",
    "model = Softmax_Regression(alpha=0.1, max_iter=5000, l2=0.1)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred1 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using l2 = 1\n",
    "model = Softmax_Regression(alpha=0.1, max_iter=5000, l2=1)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred2 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of using l2 = 0: 1.0\n",
      "Score of using l2 = 0.1: 0.9555555555555556\n",
      "Score of using l2 = 1: 0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "print('Score of using l2 = 0:', np.mean(y_pred == y_test))\n",
    "print('Score of using l2 = 0.1:', np.mean(y_pred1 == y_test))\n",
    "print('Score of using l2 = 1:', np.mean(y_pred1 == y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
