{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear_Regression:\n",
    "    def __init__(self, alpha=0.0001, max_iter=1000, gradient_descent=False):\n",
    "        ''' \n",
    "        Class constructor\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha: the learning rate determines how big the step would be on each iteration.\n",
    "        max_iter: number of times update weight\n",
    "        gradient_descent: to check if the algorithm use gradient descent or not\n",
    "        '''\n",
    "        self.alpha = alpha\n",
    "        self.max_iter = max_iter\n",
    "        self.gradient_descent = gradient_descent\n",
    "        self.weight = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains Linear Regression on the dataset (X, y).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array, shape (m, n)\n",
    "        The matrix of inputs\n",
    "        y : numpy array, shape (m, 1) \n",
    "        The vector of outputs.\n",
    "        '''\n",
    "            \n",
    "        # First column of this matrix is all ones (corresponding to x_0).\n",
    "        X = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
    "        m, n = X.shape\n",
    "        \n",
    "        # Initialize weights shape (n + 1, 1)\n",
    "        self.weight =  np.zeros((n, 1))\n",
    "        \n",
    "        # Use normal equation\n",
    "        if self.gradient_descent == False:\n",
    "            self.weight = np.linalg.pinv(X.T @ X) @ (X.T @ y)\n",
    "        # Use gradient descent\n",
    "        else:\n",
    "            for iter in range(self.max_iter):\n",
    "                error = (X @ self.weight) - y\n",
    "                grad = X.T.dot(error) / m\n",
    "                self.weight -= self.alpha * grad\n",
    "                \n",
    "                #Early stopping\n",
    "                if np.linalg.norm(self.weight, 2) < 1e-6:\n",
    "                    break\n",
    "                    \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict using the linear model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array, shape (m, n)\n",
    "        The matrix of inputs\n",
    "        \n",
    "        Return\n",
    "        ----------\n",
    "        Returns predicted values.\n",
    "        '''\n",
    "        # First column of this matrix is all ones (corresponding to x_0).\n",
    "        X = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
    "        return X @ self.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardScaler(X):\n",
    "    return (X - np.mean(X)) / np.std(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2(y_test, y_pred):\n",
    "    base = np.sum((y_pred - y_test.mean()) ** 2)\n",
    "    mse = np.sum((y_pred - y_test) ** 2)\n",
    "    r2 = 1 - (mse / base)\n",
    "    return r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read data\n",
    "data = pd.read_excel('Data/Folds5x2_pp.xlsx').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[:,:4]\n",
    "y = data[:,-1].reshape(-1,1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using normal equation\n",
    "model = Linear_Regression()\n",
    "\n",
    "X_train = standardScaler(X_train)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_test = standardScaler(X_test)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Using gradient descent\n",
    "model1 = Linear_Regression(alpha=0.4, max_iter=150000, gradient_descent=True)\n",
    "\n",
    "model1.fit(X_train, y_train)\n",
    "\n",
    "y_pred1 = model1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of using normal equation 0.9265750574674942\n",
      "Score of using gradient descent 0.9251583422643632\n"
     ]
    }
   ],
   "source": [
    "print('Score of using normal equation', r2(y_test, y_pred))\n",
    "print('Score of using gradient descent', r2(y_test, y_pred1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
