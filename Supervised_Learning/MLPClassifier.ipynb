{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier:\n",
    "    def __init__(self, hidden_layer_sizes=[10], alpha=0.0001, batch=32, max_iter=1000, l2=0.01, tol = 1e-4, random_state=42):\n",
    "        ''' \n",
    "        Class constructor\n",
    "        We use the sigmoid activation in the hidden layers, and the softmax activation at the output.\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha: the learning rate determines how big the step would be on each iteration.\n",
    "        batch: Using batch samples for one times update weight\n",
    "        max_iter: number of times update weight\n",
    "        l2: l2 regularization\n",
    "        tol: weight threshold changes to stopping.\n",
    "        '''\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.alpha = alpha\n",
    "        self.batch = batch\n",
    "        self.max_iter = max_iter\n",
    "        self.l2 = l2\n",
    "        self.tol = tol\n",
    "        self.random_state = random_state\n",
    "        self.Ws = None\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        '''\n",
    "        Computes sigmoid function for each element of array x.\n",
    "        '''\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def gradientSigmoid(self, As, Ws ,mb_X, delta, i):\n",
    "        '''\n",
    "        Computes gradient vector for sigmoid layer.\n",
    "        '''\n",
    "        temp = delta.copy()\n",
    "        delta = []\n",
    "        grad = []\n",
    "        for j in range(i, 0, -1):\n",
    "            temp = np.multiply(np.dot(temp, self.Ws[j].T), As[j] - np.power(As[j],2))\n",
    "            temp = temp[:,1:]\n",
    "            delta.append(temp)\n",
    "            grad.append(np.dot(As[j-1].T,temp) / len(As[j-1]))\n",
    "        return delta, grad\n",
    "    \n",
    "    \n",
    "    def softmax(self, z):\n",
    "        '''\n",
    "        Computes softmax function for each row of array z.\n",
    "        '''\n",
    "        # Subtract the largest value in that column to fix overflow exp.\n",
    "        A = np.exp(z - np.max(z, axis = 1, keepdims = True))\n",
    "        A /= A.sum(axis = 1, keepdims=True)\n",
    "\n",
    "        return A\n",
    "    \n",
    "    def gradientSoftmax(self, As, x, y):\n",
    "        '''\n",
    "        Computes gradient vector for softmax layer\n",
    "        '''\n",
    "        delta = As[-1] - y\n",
    "        grad = (As[-2].T @ delta)/ len(x)\n",
    "        return delta, grad\n",
    "\n",
    "    def computeForwardPropagation(self, Ws, X, need_all_layer_outputs):\n",
    "        '''\n",
    "        Computes the outputs of Neural Net by forward propagating X through the network.\n",
    "        '''\n",
    "        num_layers = len(self.Ws)\n",
    "        As = [X]\n",
    "        a_1 = X\n",
    "        for i in range(num_layers - 1):\n",
    "            z = np.dot(a_1, self.Ws[i])  \n",
    "            a = self.sigmoid(z)\n",
    "            a = np.append(np.ones((a.shape[0], 1)), a, axis=1)\n",
    "            As.append(a)\n",
    "            a_1 = a\n",
    "        z_last = np.dot(As[-1], self.Ws[-1])\n",
    "        a_last = self.softmax(z_last)\n",
    "        As.append(a_last)\n",
    "        if need_all_layer_outputs:\n",
    "            return As\n",
    "        else:\n",
    "            return As[-1]\n",
    "    \n",
    "    def computeLayerSizes(self, X, Y, hid_layer_sizes):\n",
    "        num_classes = len(np.unique(Y)) # Num classes\n",
    "        layer_sizes = [X.shape[1]] + hid_layer_sizes + [num_classes]\n",
    "        return layer_sizes\n",
    "\n",
    "    def oneHotEncoding(self, Y, num_classes):\n",
    "        '''\n",
    "        Y to one hot encoding\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        y: numpy array, shape (m, 1) \n",
    "        The vector of outputs.\n",
    "        \n",
    "        Return\n",
    "        y: numpy array, shape (m, num_classes) \n",
    "        '''\n",
    "        one_hot_Y = np.zeros((len(Y), num_classes))\n",
    "        one_hot_Y[np.arange(len(Y)), Y.reshape(-1)] = 1\n",
    "        return one_hot_Y\n",
    "    \n",
    "    def initWeight(self, X, Y, layer_sizes):\n",
    "        np.random.seed(self.random_state) \n",
    "        self.Ws = np.array([np.random.randn(layer_sizes[i] + 1 , layer_sizes[i + 1]) / np.sqrt(layer_sizes[i] + 1) \n",
    "              for i in range(len(layer_sizes) - 1)]) \n",
    "    \n",
    "    def updateWeights(self, Ws, As, mb_X, mb_Y, alpha):\n",
    "        \n",
    "        #update weights for softmax layer\n",
    "        num_hidden_layer = len(self.Ws) - 1\n",
    "        delta_last , grad_last = self.gradientSoftmax(As, mb_X, mb_Y)\n",
    "        delta , grad = self.gradientSigmoid(As, self.Ws , mb_X, delta_last, num_hidden_layer)\n",
    "        self.Ws[-1] -= alpha * (grad_last  + self.l2 * grad_last) / len(mb_X)\n",
    "        \n",
    "        #update weights for hidden layer\n",
    "        grad = grad[::-1]\n",
    "        for i in range(len(delta)):\n",
    "            self.Ws[i] -= alpha * (grad[i] + self.l2 * grad[i]) / len(mb_X)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        '''\n",
    "        Trains Softmax Regression on the dataset (X, y) using mini-batch Gradient Descent.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array, shape (m, n)\n",
    "        The matrix of inputs\n",
    "        y : numpy array, shape (m, 1) \n",
    "        The vector of outputs.\n",
    "        '''\n",
    "        # Get layer sizes\n",
    "        layer_sizes = self.computeLayerSizes(X, y, self.hidden_layer_sizes)\n",
    "        \n",
    "        # Prepare for training\n",
    "        self.initWeight(X, y, layer_sizes)\n",
    "        y = self.oneHotEncoding(y, layer_sizes[-1])\n",
    "    \n",
    "        # First column of this matrix is all ones (corresponding to x_0).\n",
    "        X = np.append(np.ones((X.shape[0], 1)), X, axis=1)\n",
    "        m, n = X.shape\n",
    "        \n",
    "        # Checking for changes in weight after 20 iter \n",
    "        check_w_after = 20\n",
    "        \n",
    "        for iter in range(1, self.max_iter + 1):\n",
    "            # mix data \n",
    "            mix_id = np.random.permutation(m)\n",
    "            for i in list(range(0, m, self.batch)):\n",
    "                # Get batch samples\n",
    "                mb_X = X[mix_id[i : i + self.batch]]\n",
    "                mb_Y = y[mix_id[i : i + self.batch]]\n",
    "                \n",
    "                # Compute forward propagation (all layers)\n",
    "                As = self.computeForwardPropagation(self.Ws, mb_X, True)\n",
    "                \n",
    "                # Back propagation, compute each layer's gradient and update its W\n",
    "                self.updateWeights(self.Ws, As, mb_X, mb_Y, self.alpha)             \n",
    "        \n",
    "    def predict(self, X):\n",
    "        '''\n",
    "        Predict using the Softmax Regression model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : numpy array, shape (m, n)\n",
    "        The matrix of inputs\n",
    "        \n",
    "        Return\n",
    "        ----------\n",
    "        Returns predicted values.\n",
    "        '''\n",
    "        # First column of this matrix is all ones (corresponding to x_0).\n",
    "        X = np.append(np.ones((X.shape[0], 1)), X, axis = 1)    \n",
    "        # Compute training info, save it, and print it\n",
    "        A = self.computeForwardPropagation(self.Ws, X, False)\n",
    "        return np.argmax((A), axis = 1).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardScaler(X):\n",
    "    return (X - np.mean(X)) / np.std(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "\n",
    "X = iris.data\n",
    "y = iris.target.reshape(-1,1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 1 hidden layer with 50 units\n",
    "model = MLPClassifier(hidden_layer_sizes=[50], alpha=0.2)\n",
    "\n",
    "X_train = standardScaler(X_train)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "X_test = standardScaler(X_test)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 1 hidden layer with 100 units\n",
    "model = MLPClassifier(hidden_layer_sizes=[100], alpha=0.2)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred1 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 2 hidden layers with (50,50) units\n",
    "model = MLPClassifier(hidden_layer_sizes=[50,50], alpha=0.2)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred2 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using 2 hidden layers with (100,100) units\n",
    "model = MLPClassifier(hidden_layer_sizes=[100,100], alpha=0.2)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred3 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score of using 1 hidden layer with 50 units: 0.9111111111111111\n",
      "Score of using 1 hidden layer with 100 units: 1.0\n",
      "Score of using 2 hidden layers with (50,50) units: 0.9333333333333333\n",
      "Score of using 2 hidden layers with (100,100) units: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "print('Score of using 1 hidden layer with 50 units:', np.mean(y_pred == y_test))\n",
    "print('Score of using 1 hidden layer with 100 units:', np.mean(y_pred1 == y_test))\n",
    "print('Score of using 2 hidden layers with (50,50) units:', np.mean(y_pred2 == y_test))\n",
    "print('Score of using 2 hidden layers with (100,100) units:', np.mean(y_pred3 == y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
